llm:
  model: "gpt-4.1-mini"
  base_url: "${OPENAI_BASE_URL:-https://api.openai.com/v1}"
  api_key: "${OPENAI_API_KEY}"
  temperature: 1.0
  max_tokens: 1024

params:
  index: "msmarco-v1-passage"
  retrieval_k: 10
  k1: 0.9
  b: 0.4
  rm3: false
  rocchio: false
  rocchio_use_negative: false
  disable_bm25_param: true
  batch_size: 128
  threads: 16
  parallel: false  # Enable parallel generation for methods like MuGI

# Method-specific default parameters
genqr:
  llm:
    temperature: 0.8
    max_tokens: 256
  params:
    n_generations: 5  # Number of times to call LLM for keywords

genqr_ensemble:
  llm:
    temperature: 0.92  # Nucleus sampling (GenQREnsemble paper)
    max_tokens: 256
  params:
    parallel: false  # Sequential generation by default (set true for parallel)

mugi:
  llm:
    temperature: 1.0  # High temperature for diversity (MuGI paper)
    max_tokens: 1024
  params:
    num_docs: 5  # Number of pseudo-documents to generate
    adaptive_times: 5  # Adaptive query repetition factor
    parallel: false  # Sequential generation by default (set true for parallel)
    mode: "zs"  # "zs" (zero-shot) or "fs" (few-shot)

seed: 17
retries: 2
